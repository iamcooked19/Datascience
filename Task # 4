import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load Boston Housing Dataset manually
data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# Column names (from original dataset)
column_names = [
    'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS',
    'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT'
]
X = pd.DataFrame(data, columns=column_names)
y = pd.Series(target, name='PRICE')

# Normalize numerical features
X = (X - X.mean()) / X.std()

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=0.2, random_state=42)

# -------------------------
# Linear Regression (from scratch)
# -------------------------
class LinearRegressionScratch:
    def fit(self, X, y):
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        self.theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

    def predict(self, X):
        X_b = np.c_[np.ones((X.shape[0], 1)), X]
        return X_b @ self.theta

# -------------------------
# Random Forest (from scratch, simplified)
# -------------------------
class DecisionTreeRegressorScratch:
    def __init__(self, depth=3):
        self.depth = depth

    def fit(self, X, y):
        self.feature_index = np.random.randint(0, X.shape[1])
        self.threshold = np.median(X[:, self.feature_index])
        left = X[:, self.feature_index] < self.threshold
        right = ~left
        self.left_value = np.mean(y[left]) if np.any(left) else np.mean(y)
        self.right_value = np.mean(y[right]) if np.any(right) else np.mean(y)

    def predict(self, X):
        return np.where(X[:, self.feature_index] < self.threshold, self.left_value, self.right_value)

class RandomForestScratch:
    def __init__(self, n_estimators=10, max_depth=3):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.trees = []

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_estimators):
            idxs = np.random.choice(len(X), len(X), replace=True)
            tree = DecisionTreeRegressorScratch(depth=self.max_depth)
            tree.fit(X[idxs], y[idxs])
            self.trees.append(tree)

    def predict(self, X):
        preds = np.array([tree.predict(X) for tree in self.trees])
        return np.mean(preds, axis=0)

# -------------------------
# XGBoost (from scratch, simplified gradient boosting)
# -------------------------
class XGBoostScratch:
    def __init__(self, n_estimators=10, learning_rate=0.1):
        self.n_estimators = n_estimators
        self.learning_rate = learning_rate
        self.trees = []
        self.initial_pred = 0

    def fit(self, X, y):
        self.initial_pred = np.mean(y)
        pred = np.full_like(y, self.initial_pred)
        for _ in range(self.n_estimators):
            residual = y - pred
            tree = DecisionTreeRegressorScratch()
            tree.fit(X, residual)
            update = tree.predict(X)
            pred += self.learning_rate * update
            self.trees.append(tree)

    def predict(self, X):
        pred = np.full((X.shape[0],), self.initial_pred)
        for tree in self.trees:
            pred += self.learning_rate * tree.predict(X)
        return pred

# -------------------------
# Performance & Visualization
# -------------------------
def evaluate(model, X_train, y_train, X_test, y_test, name="Model"):
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"{name} RMSE: {rmse:.2f}, R2: {r2:.2f}")
    return rmse, r2

def plot_feature_importance(model, name, feature_names):
    importances = np.zeros(X.shape[1])
    for tree in model.trees:
        importances[tree.feature_index] += 1
    plt.barh(feature_names, importances)
    plt.title(f"{name} - Feature Importance")
    plt.xlabel("Importance Score")
    plt.show()

# Instantiate and evaluate models
lr_model = LinearRegressionScratch()
lr_model.fit(X_train, y_train)
evaluate(lr_model, X_train, y_train, X_test, y_test, "Linear Regression")

rf_model = RandomForestScratch(n_estimators=20)
rf_model.fit(X_train, y_train)
evaluate(rf_model, X_train, y_train, X_test, y_test, "Random Forest")
plot_feature_importance(rf_model, "Random Forest", X.columns)

xgb_model = XGBoostScratch(n_estimators=20)
xgb_model.fit(X_train, y_train)
evaluate(xgb_model, X_train, y_train, X_test, y_test, "XGBoost")
plot_feature_importance(xgb_model, "XGBoost", X.columns)
